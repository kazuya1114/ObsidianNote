回帰モデルは、給料といった値を予測する際に使われる。  
例えば、独立変数が時間だとすると、回帰モデルを作成することによって将来のある値を予測することになる。  
回帰の手法は、線形回帰からサポートベクトル回帰、ランダムフォレストまで多くの種類がある。  

# 単回帰分析

![単回帰分析](../pic/2.png)
4年目の年収を予測するには、判明しているデータ（今回だったら5つの点）に一番近い直線（=回帰直線）を引き、X軸・Y軸の接点の値が予測値となる。  
`y = b0 + b1X`
b0：y軸との切片の値  
b1：就業年数が1年増えたら年収がどれだけ増えるか（係数）
データ：年収  
アルゴリズム：直線をどのように引くか

## 回帰直線を求めるアルゴリズム
### 最小二乗法

![最小二乗法](../pic/3.png) 
式：`i∑​(yi​−y^​i​)2`  
∑(yi​−(axi​+b))2（=誤差）を最小にする **a（傾き）と b（切片）を数学的に直接求める方法**。

### 勾配降下法

∑(yi​−(axi​+b))2（=誤差）を最小にする **a と b を求める方法**だが、最小二乗法とは異なり、初期値からスタートし傾きを見ながらパラメータを少しずつ更新して最適な a（傾き）とb（切片）に近づけるという **最適化アルゴリズム**。

## 単回帰分析の実装

```python
# 単回帰分析
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# 分析前の前処理
dataset = pd.read_csv('data/Salary_Data.csv')
X = dataset.iloc[:, :-1].values  # 独立変数（YearsExperience列）
Y = dataset.iloc[:, -1].values   # 従属変数（Salary列)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1/3, random_state=0) # データ分割

# 単回帰分析の実行（データセットを使った学習）
regressor = LinearRegression()
regressor.fit(X_train, Y_train) # 学習（回帰直線の決定（＝切片と傾きの決定））

# テストデータを使った予測
y_pred = regressor.predict(X_test) # 予測値の算出

# 結果の可視化
plt.scatter(X_train, Y_train, color='red') # 学習用データの散布図を描画
plt.plot(X_train, regressor.predict(X_train), color='blue') # 回帰直線を散布図に描画
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show() # 学習用データのプロット表示

# テスト用データの可視化
plt.scatter(X_test, Y_test, color='red') # テスト用データの散布図を描画
plt.plot(X_train, regressor.predict(X_train), color='blue') # 回帰直線を散布図に描画
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show() # テスト用データのプロット表示
```

