機械学習のモデルに対して入力するデータについて、データの補完（欠損の補完）などを行い、データを整理する。

# 前処理の種類

- 欠損値の処理

# ライブラリについて

- numpy：演算で使う
- matplotlib：データを可視化する際に使う
- pandas：データを整理するのに使う

`pandasでデータ整理（前処理）（numpyの形式に変換する）　⇒　numpyで演算　⇒　matplotlibで可視化`

```python
import numpy as np
import matplotlib.pylot as plt
import pandas as pd
```

# 独立変数（X）と従属変数（Y）

機械学習：モデルを作る
  - モデルはアルゴリズムから作られる
  - アルゴリズムを作るにはデータが必要

##### 独立変数（X)：入力データ
##### 従属変数（Y）：出力データ

# ilocメソッド

数字を指定することで対象のデータを抽出する。  
X（独立変数） = dataset.iloc{:, :-1}.values  
Y（従属変数） = dataset.iloc{:, -1}.values  
※iloc(行, 列)となる  
※「：」が指定された場合、全てが対象となる  
※列に「：-1」と指定された場合、-1のインデックスより左側（Salary、Age、Country）が対象となる（0はPurchasedの列の場所（=一番右側の列））

| Country | Age | Salary | Purchased |
| ------- | --- | ------ | --------- |
| France  | 44  | 72000  | No        |
| Spain   | 27  | 48000  | Yes       |
| Germany | 30  | 54000  | No        |
| Spain   | 38  | 61000  | No        |

# データセットのインポート

目的に基づいたデータを得るために元となるデータをセットする
```python
# データセットの読み込み
dataset = pd.read_csv('path/to/data.csv')
```

# 読み込んだデータセットを独立変数(X)と従属変数(Y)に振り分ける

```python
# .valuesはデータの型をdataframe型からndarray型（scikit-learnで使用）に変換する

# 独立変数（Data.csvファイルのPurchased列以外の前列全行が対象）
X = dataset.iloc[:, :-1].values

# 従属変数（Data.csvファイルのPurchased列の全行が対象）
Y = dataset.iloc[:, -1].values
```

# 欠損値の処理１（欠損データを削除する）

欠損データが全体の1%未満ならこの処理をする（一般論のため、数値は多少上下する）。

# 欠損値の処理２（欠損データを補完する）

1. 平均値で補完する
2. 最頻値（一番数が多いデータ）で補完する
3. 中央値（データを順に並べたときに真ん中に来るデータ）で補完する

#### 平均値で補完する
```python
from sklearn.impute import SimpleImputer

# 　missing_values=np.nan：欠損値をNot a Numberとして扱う
# 　strategy='mean'：欠損値を各列の平均値で補完する
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')

# fitメソッドでXの各列の平均値を計算し、transformメソッドで欠損値を平均値で補完する
imputer.fit(X[:, :1:3]) # fit：Age、Salary列の全行に対して適用
X[:, 1:3] = imputer.transform(X[:, 1:3]) # transformメソッドで欠損値を平均値で補完
```

# 変数のエンコーディング

エンコーディング：`機械学習でデータを使えるようにすること（数字じゃないと演算ができない）`

#### 数字以外のデータのエンコーディングの種類

  - バイナリエンコーディング：`対象データをバイナリ（２進数）に変換する`
    ワンホットエンコーディングよりも次元数が少なくなり、カテゴリ数が多い場合に有効です。
    ##### 仕組み
    1. 各カテゴリに整数IDを割り当てます。
    2. そのIDを2進数に変換します。
    3. 2進数の各桁を新しい特徴量として使います。
    ##### 例：カテゴリ: `["A", "B", "C", "D"]`
    - A → 0 → 00
    - B → 1 → 01
    - C → 2 → 10
    - D → 3 → 11
    2桁のバイナリで表現されます。
    ##### 実装例
```python
    # バイナリエンコーディングの例
import category_encoders as ce

encoder = ce.BinaryEncoder(cols=['Country'])
X_encoded = encoder.fit_transform(dataset[['Country']])
print(X_encoded.head())
```

  - ワンホットエンコーディング：`対象データを 「0と1」だけで表現するベクトルに変換する`
    例えば、`["赤", "青", "緑"]` という3つのカテゴリがあれば、
    - 赤 → [1, 0, 0]
    - 青 → [0, 1, 0]
    - 緑 → [0, 0, 1]
    これにより、カテゴリ間に「順序」や「大小関係」が生まれず、モデルが正しく学習できます。
    ##### 実装例
  ```python
from sklearn.preprocessing import OneHotEncoder

# 例: 国名の列をワンホットエンコーディング
onehotencoder = OneHotEncoder()
X_country = onehotencoder.fit_transform(X[:, 0].reshape(-1, 1)).toarray()

# pandasのget_dummiesでワンホットエンコーディング
X = pd.get_dummies(X, columns=[0])
  ```

# 独立変数のエンコーディング

```python
from sklearn.compose import ColumnTranssformer
from sklearn.preprocessing import OneHotEncoder

# 独立変数のエンコーディング
# 国名の列をワンホットエンコーディング
# 　'encoder'：変換器の名前（任意名称）
# 　[0]：エンコーディングする列のインデックス（ここでは0指定のため国名の列が対象）
# 　reminder='passthrough'：指定しなかった列はそのまま残す
ct = ColumnTranssformer(transformers=[('encoder', OneHotEncoder(), [0])], reminder='passthrough')

X = np.array(ct.fit_transform(X)) # fitとtransformを同時に実行（Xが指定されたため、Country～Salary列）。np.arraayでndarray型に変換（基本的には不要。おまじない）
```

# 従属変数のエンコーディング

```python
from sklearn.preprocessing import LabelEncoder

# 従属変数のエンコーディング
le = LabelEncoder()
y = le.fit_transform(Y) # Y列（Purchased）のYes/Noを0/1に変換
```

# 訓練用データセットとテストデータセットへの分割

#### 訓練用データセットとテスト用データセットへ分割する理由

1. テストデータに前処理の影響が漏れるのを防ぐため（データリーク防止）
2. 評価の公平性・汎化性能の測定
3. 実運用と同じ流れを再現するため

### 本資料では、欠損値処理⇒変数のエンコーディング⇒データセット（訓練用、テスト用への分割）の順に学習しているが、`実務ではデータセット（訓練用、テスト用への分割）⇒欠損値処理⇒変数のエンコーディングの順に行う`
#### 実装例
```python
from sklearn.model_selection import train_test_split

# 訓練用データとテスト用データセットへの分割
# 　test_size=0.2：データ全体の20%をテスト用データに使用（残りの80%を訓練用データに使用）
# 　random_state=0：乱数シードを0に設定（毎回同じ分割結果になるようにするための設定）
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)
```

# フィーチャースケーリング

標準化と正規化。  
元のデータが標準正規分布に従っている場合は標準化をした方がいい。   
※`状況に応じては不要（例：単回帰分析）`

#### フィーチャースケーリングをした方がいい理由（クラスタリング）


#### クラスタリング：**データを「似ているもの同士」でグループ（クラスタ）に自動的に分ける分析手法**のこと

#### 標準化：平均=0、標準偏差=1の形にデータを変換する
#### 正規化：最小=0、最大=1の範囲にデータを収める

```python
from sklearn.preprocessing import StandardScaler

# フィーチャースケーリング（特徴量の標準化）
sc = StandardScaler()
X_train[:, 3:] = sc.fit_transform(X_train[:, 3:]) # 訓練用データの標準化(Age～Salary列)
X_test[:, 3:] = sc.transform(X_test[:, 3:])       # テスト用データの標準化(Age～Salary列)
```

